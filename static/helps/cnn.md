# æ–‡æœ¬åˆ†ç±»--CNN
# 1.TxetCNNæ•°æ®é¢„å¤„ç†
## 1.1 è¯å‘é‡
æ‰“ç®—è‡ªå·±è®­ç»ƒè¯å‘é‡çš„åŒå­¦ï¼Œå¯ä»¥ä½¿ç”¨gensimï¼Œæ–¹ä¾¿å¿«æ·ï¼Œå½“ç„¶ä½¿ç”¨tensorflowæ¥åšä¹Ÿæ˜¯å¯ä»¥çš„ã€‚ä¸‹é¢æ˜¯ä½¿ç”¨gensimè®­ç»ƒè¯å‘é‡çš„ä»£ç ã€‚
```
#encoding=utf-8
from gensim.models.word2vec import Word2Vec
form gensim.models.word2vec import LineSentence

sentences = LineSentence('WordSeg.text_utf-8')
model = 
```
sizeæ˜¯è¯å‘é‡çš„ç»´åº¦ï¼Œsg=0,æ˜¯ç”¨cbowè¿›è¡Œè®­ç»ƒï¼Œsg=1,ä½¿ç”¨sgè¿›è¡Œè®­ç»ƒã€‚

# 1.2 æ–‡æœ¬åˆ†è¯
æœ‰äº†æ‰“æ ‡ç­¾çš„æ–‡æœ¬ï¼Œæ¥ä¸‹æ¥å½“ç„¶æ˜¯è¦å¤„ç†å®ƒäº†å•Š
```
def read_file(filename):
    '''ä¸­æ–‡åˆ†è¯ï¼šå°†ä¸­æ–‡å¥å­åˆ†è¯è¯ç»„
    '''
    re_han = re.compile(u"([\u4E00-\u9FD5a-zA-Z0-9+#&\._%]+)")  # the method of cutting text by punctuation
    contents, labels = [], []
    with codecs.open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                line = line.rstrip()
                assert len(line.split('\t')) == 2   # å…±æœ‰ä¸¤åˆ— ç¬¬ä¸€åˆ—ä¸ºæ ‡ç­¾ï¼Œç¬¬äºŒåˆ—ä¸ºæ–‡æœ¬
                label, content = line.split('\t')
                labels.append(label)
                blocks = re_han.split(content)
                word = []
                for blk in blocks:
                    if re_han.match(blk):
                        word.extend(jieba.lcut(blk))
                contents.append(word)
            except:
                pass
    return labels, contents  #è¿”å› æ ‡ç­¾ å’Œ è¯ç»„

```
è¿™æ­¥çš„æ“ä½œä¸»è¦æ˜¯å¯¹æ–‡æœ¬åˆ†è¯ï¼Œç„¶åå¾—åˆ°æ–‡æœ¬åˆ—è¡¨ï¼Œæ ‡ç­¾åˆ—è¡¨ã€‚ä¸¾ä¸ªğŸŒ°ã€‚

content=[['æ–‡æœ¬','åˆ†è¯'],['æ ‡ç­¾','åˆ—è¡¨']ï¼›label=['A','B']

## 1.3 å»ºç«‹è¯å…¸ï¼Œè¯å…¸è¯å‘é‡
ä¸èƒ½æ˜¯ä¸ªè¯æˆ‘å°±è¦å§ã€‚é‚£æ€ä¹ˆåŠå‘¢ï¼Ÿå»åœç”¨è¯ï¼å»äº†åœç”¨è¯ä¹‹åï¼Œå–æ–‡æœ¬(è¿™ä¸ªæ–‡æœ¬æŒ‡çš„æ˜¯æ‰€æœ‰æ–‡æœ¬ï¼ŒåŒ…æ‹¬è®­ç»ƒã€æµ‹è¯•ã€éªŒè¯é›†)ä¸­å‰Nä¸ªè¯ï¼Œè¡¨ç¤ºè¿™Nä¸ªè¯æ˜¯æ¯”è¾ƒé‡è¦çš„ï¼Œç„¶åä¿å­˜ã€‚ä¹‹å‰è®­ç»ƒçš„è¯å‘é‡æ˜¯ä¸ªæ•°æ®é‡å¾ˆå¤§é›†åˆã€‚å¾ˆå¤šè¯ï¼Œæˆ‘å·²ç»ä¸éœ€è¦äº†ï¼Œæˆ‘åªè¦è¿™Nä¸ªè¯çš„è¯å‘é‡ã€‚åŒæ ·æ˜¯ä¸Šä»£ç ã€‚
```
def built_vocab_vector(dataSet, filenames, voc_size = 10000):
    '''
    å»åœç”¨è¯ï¼Œå¾—åˆ°å‰9999ä¸ªè¯ï¼Œè·å–å¯¹åº”çš„è¯ ä»¥åŠ è¯å‘é‡
    :param filenames:
    :param voc_size:
    :return:
    '''
    stopword_file = os.path.join(MEDIA_ROOT,os.path.normpath('data/cnn/stopwords.txt'))  # ä¸­æ–‡åœç”¨è¯ å…±ç”¨

    stopword = open(stopword_file, 'r', encoding='utf-8')
    stop = [key.strip(' \n') for key in stopword]

    all_data = []
    j = 1
    embeddings = np.zeros([10000, 100])
    categories = []  # ç±»åˆ«

    for filename in filenames:
        labels, content = read_file(filename)  #åœ¨è¿™é‡Œå°±å¼€å§‹åˆ†è¯äº†ï¼ˆread_file)
        if labels not in categories:
            categories.append(labels)

        for eachline in content:
            line =[]
            for i in range(len(eachline)):
                if str(eachline[i]) not in stop:#å»åœç”¨è¯
                    line.append(eachline[i])
            all_data.extend(line)

    counter = Counter(all_data)
    count_paris = counter.most_common(voc_size-1)
    word, _ = list(zip(*count_paris))
    # f_file = os.path.join(MEDIA_ROOT, os.path.normpath('data/cnn/vector_word.txt'))  #äº§ç”Ÿè®­ç»ƒé›†çš„è¯å‘é‡è¡¨æ–‡ä»¶
    f_file = os.path.join(MEDIA_ROOT, 'data','vector_word.txt')#  è¯å‘é‡
    dataSet.vector_word_filename = f_file
    f = codecs.open(f_file, 'r', encoding='utf-8')
    # vocab_word_file = os.path.join(MEDIA_ROOT, os.path.normpath('data/cnn/vocab_word.txt'))
    vocab_word_file = os.path.join(MEDIA_ROOT, 'data',str(dataSet.id)+'.vocab_word.txt')
    dataSet.vocab_filename = vocab_word_file
    vocab_word = open(vocab_word_file, 'w', encoding='utf-8')
    for ealine in f:
        item = ealine.split(' ')
        key = item[0]
        vec = np.array(item[1:], dtype='float32')
        if key in word:
            embeddings[j] = np.array(vec)
            vocab_word.write(key.strip('\r') + '\n')
            j += 1
    # np_file = os.path.join(MEDIA_ROOT, os.path.normpath('data/cnn/vector_word.npz'))
    np_file = os.path.join(MEDIA_ROOT, 'data',str(dataSet.id)+'.vector_word.npz')
    dataSet.vector_word_npz = np_file
    np.savez_compressed(np_file, embeddings=embeddings)

    return categories
```
æˆ‘æå–äº†æ–‡æœ¬çš„å‰9999ä¸ªæ¯”è¾ƒé‡è¦çš„è¯ï¼Œå¹¶æŒ‰é¡ºåºä¿å­˜äº†ä¸‹æ¥ã€‚embeddings= np.zeros([10000, 100]) è¡¨ç¤ºæˆ‘å»ºç«‹äº†ä¸€ä¸ª10000ä¸ªè¯ï¼Œç»´åº¦æ˜¯100çš„è¯å‘é‡é›†åˆã€‚ç„¶åå°†9999ä¸ªè¯åœ¨å¤§è¯å‘é‡ä¸­çš„æ•°å€¼ï¼ŒæŒ‰1-9999çš„é¡ºåºï¼Œæ”¾å…¥äº†æ–°å»ºçš„è¯å‘é‡ä¸­ã€‚ç¬¬0é¡¹ï¼Œè®©å®ƒä¿æŒæ˜¯100ä¸ª0çš„çŠ¶æ€

## 1.4  å»ºç«‹è¯å…¸
```
def get_wordid(filename):
    key = open(filename, 'r', encoding='utf-8')

    wordid = {}
    wordid['<PAD>'] = 0
    j = 1
    for w in key:
        w = w.strip('\n')
        w = w.strip('\r')
        wordid[w] = j
        j += 1
    return wordid
```
æ³¨æ„ï¼šè¯å…¸é‡Œé¢è¯çš„é¡ºåºï¼Œè¦è·Ÿæ–°å»ºçš„è¯å‘é‡ä¸­è¯çš„é¡ºåºä¸€è‡´

## 1.5 æ ‡ç­¾è¯å…¸
```

def read_category(categories):
    # categories = ['ä½“è‚²', 'è´¢ç»', 'æˆ¿äº§', 'å®¶å±…', 'æ•™è‚²', 'ç§‘æŠ€', 'æ—¶å°š', 'æ—¶æ”¿', 'æ¸¸æˆ', 'å¨±ä¹']  # æš‚æ—¶å†™æ­»
    cat_to_id = dict(zip(categories, range(len(categories))))
    return categories, cat_to_id
```
å°†æ ‡ç­¾ä¹Ÿè¯å…¸ä¸€ä¸‹ã€‚

## 1.6 Paddingçš„è¿‡ç¨‹
paddingæ˜¯å°†æ‰€æœ‰å¥å­è¿›è¡Œç­‰é•¿å¤„ç†ï¼Œä¸å¤Ÿçš„åœ¨å¥å­æœ€åè¡¥0ï¼›å°†æ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç ã€‚

```
def process(filename, word_to_id, cat_to_id, max_length=600):
    """
    Args:
        filename:train_filename or test_filename or val_filename
        word_to_id:get from def read_vocab()
        cat_to_id:get from def read_category()
        max_length:allow max length of sentence
    Returns:
        x_pad: sequence data from  preprocessing sentence
        y_pad: sequence data from preprocessing label

    """
    labels, contents = read_file(filename)
    data_id, label_id = [], []
    for i in range(len(contents)):
        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])
        label_id.append(cat_to_id[labels[i]])
    x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length, padding='post', truncating='post')
    y_pad = kr.utils.to_categorical(label_id)
    return x_pad, y_pad
```
é¦–å…ˆå°†å¥å­ä¸­çš„è¯ï¼Œæ ¹æ®è¯å…¸ä¸­çš„ç´¢å¼•ï¼Œå˜æˆå…¨æ•°å­—çš„å½¢å¼ï¼›æ ‡ç­¾ä¹Ÿè¿›è¡ŒåŒæ ·å¤„ç†ã€‚ç„¶åï¼Œæ ¹æ®max_length(å¥å­æœ€å¤§é•¿åº¦)è¿›è¡Œpadding,å¾—åˆ°x_pad,æ ‡ç­¾è½¬æ¢one-hotæ ¼å¼ã€‚å¥½äº†ï¼Œåˆ°è¿™é‡Œæ–‡æœ¬çš„é¢„å¤„ç†ï¼Œå‘Šä¸€æ®µè½ï¼
## 1.7 è¯»å–æ‰€éœ€æ•°æ®
æˆ‘ä»¬ä¿å­˜äº†10000è¯çš„è¯å‘é‡ï¼Œæˆ‘ä»¬è¦è¯»å–å®ƒï¼Œè¿˜æœ‰å¤„ç†çš„å¥å­ï¼Œæˆ‘ä»¬ä¹Ÿè¦åˆ†æ‰¹ï¼Œè¾“å…¥è¿›æ¨¡å‹ã€‚
```
def get_word2vec(filename):
    with np.load(filename) as data:
        return data["embeddings"]


def batch_iter(x, y, batch_size = 64):
    data_len = len(x)
    num_batch = int((data_len - 1)/batch_size) + 1
    indices = np.random.permutation(np.arange(data_len))
    '''
    np.arange(4) = [0,1,2,3]
    np.random.permutation([1, 4, 9, 12, 15]) = [15,  1,  9,  4, 12]
    '''
    x_shuff = x[indices]
    y_shuff = y[indices]
    for i in range(num_batch):
        start_id = i * batch_size
        end_id = min((i+1) * batch_size, data_len)
        yield x_shuff[start_id:end_id], y_shuff[start_id:end_id]
```
åœ¨ä»£ç é‡Œï¼Œæˆ‘ç”¨ä¸€ä¸ªä¾‹å­ï¼Œè§£é‡Šäº†np.random.permutationçš„ä½œç”¨ã€‚
# 2.tensorflowä¸­çš„TextCNN
![](cnn/textCNN.webp)  
æ¥ä¸‹æ¥å¼€å§‹æ­å»ºTextCNNåœ¨tensorflowä¸­çš„å®ç°
## 2.1 å®šä¹‰å ä½ç¬¦
```
    def __init__(self, pm):
        # éœ€è¦å¾€ä¼ pm
        self.pm = pm
        self.input_x = tf.placeholder(tf.int32, shape=[None, self.pm.seq_length], name='input_x')
        self.input_y = tf.placeholder(tf.float32, shape=[None, self.pm.num_classes], name='input_y')
        self.keep_pro = tf.placeholder(tf.float32, name='drop_out')
        self.global_step = tf.Variable(0, trainable=False, name='global_step')
        self.cnn()
```
## 2.2 embedding
```
        with tf.device('/cpu:0'), tf.name_scope('embedding'):
            self.embedding = tf.get_variable("embeddings", shape=[self.pm.vocab_size, self.pm.embedding_size],
                                             initializer=tf.constant_initializer(self.pm.pre_trianing))
            embedding_input = tf.nn.embedding_lookup(self.embedding, self.input_x)
            self.embedding_expand = tf.expand_dims(embedding_input,
                                                   -1)  # [None,seq_length,embedding_size,1] [None,600,100,1]
```
vocab_size:æ˜¯è¯çš„ä¸ªæ•°ï¼Œåœ¨è¿™é‡Œæ˜¯10000ï¼›  
embedding_sizeï¼šæ˜¯è¯å‘é‡å°ºå¯¸ï¼Œè¿™é‡Œæ˜¯100ï¼›  
embedding_lookup:æˆ‘æŠŠå®ƒçœ‹æˆä¸excel vlookupç±»ä¼¼çš„æŸ¥æ‰¾å‡½æ•°ï¼Œæ˜¯å°†embeddingä¸­çš„è¯å‘é‡æ ¹æ®input_xä¸­çš„æ•°å­—è¿›è¡Œç´¢å¼•ï¼Œç„¶åå¡«å……ã€‚æ¯”å¦‚ï¼Œinput_xä¸­çš„3ï¼Œå°†input_xä¸­çš„3ç”¨embeddingä¸­çš„ç¬¬ä¸‰è¡Œçš„100ä¸ªæ•°å­—è¿›è¡Œå¡«å……ï¼Œå¾—åˆ°ä¸€ä¸ªtensor:[batch_size,seq_length,embedding_size].  
å› ä¸ºï¼Œå·ç§¯ç¥ç»ç½‘ç»œä¸­çš„ï¼Œconv2dæ˜¯éœ€è¦4ç»´å¼ é‡çš„ï¼Œæ•…ç”¨tf.expand_dimsåœ¨embedding_inputæœ€åå†è¡¥ä¸€ç»´ã€‚

## 3.3 å·ç§¯å±‚
filte é«˜åº¦è®¾å®šä¸ºã€2ï¼Œ3ï¼Œ4ã€‘ä¸‰ç§ï¼Œå®½åº¦ä¸è¯å‘é‡ç­‰å®½ï¼Œå·ç§¯æ ¸æ•°é‡è®¾ä¸ºnum_filterã€‚å‡è®¾batch_size =1ï¼Œå³å¯¹ä¸€ä¸ªå¥å­è¿›è¡Œå·ç§¯æ“ä½œã€‚æ¯ä¸€ç§filterå·ç§¯åï¼Œç»“æœè¾“å‡ºä¸º  
[1,seq_length - filter_size +1,1,num_filter]çš„tensorã€‚å†ç”¨ksize=[1,seq_length - filter_size + 1,1,1]è¿›è¡Œmax_pooling,å¾—åˆ°[1,1,1,num_filter]è¿™æ ·çš„tensor.å°†å¾—åˆ°çš„ä¸‰ç§ç»“æœè¿›è¡Œç»„åˆ,å¾—åˆ°[1,1,1,num_filter*3]çš„tensor.æœ€åå°†ç»“æœå˜å½¢ä¸€ä¸‹[-1,num_filter*3]ï¼Œç›®çš„æ˜¯ä¸ºäº†ä¸‹é¢çš„å…¨è¿æ¥ã€‚å†æ¬¡æœ‰è¯·ä»£ç 
```
pooled_outputs = []
        for i, filter_size in enumerate(self.pm.kernel_size):
            with tf.name_scope("conv-maxpool-%s" % filter_size):
                filter_shape = [filter_size, self.pm.embedding_size, 1, self.pm.num_filters]  # [2,100,1,128]
                w = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name='w')  # å·ç§¯æ ¸
                b = tf.Variable(tf.constant(0.1, shape=[self.pm.num_filters]), name='b')  # [128]
                conv = tf.nn.conv2d(self.embedding_expand, w, strides=[1, 1, 1, 1], padding='VALID',
                                    name='conv')  # [None,599,1,128]
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')

                pooled = tf.nn.max_pool(h, ksize=[1, self.pm.seq_length - filter_size + 1, 1, 1], strides=[1, 1, 1, 1],
                                        padding='VALID', name='pool')  # æ± åŒ–çš„å‚æ•°å¾ˆç²¾å¦™   [None,1,1,128]
                pooled_outputs.append(pooled)

        num_filter_total = self.pm.num_filters * len(self.pm.kernel_size)  # 128 * 3
        self.h_pool = tf.concat(pooled_outputs, 3)
        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filter_total])  # [None, 128 *3]
```
## 3.4  å…¨è¿æ¥å±‚
åœ¨å…¨è¿æ¥å±‚ä¸­è¿›è¡Œdropout,é€šå¸¸ä¿æŒç‡ä¸º0.5ã€‚å…¶ä¸­num_classesä¸ºæ–‡æœ¬åˆ†ç±»çš„ç±»åˆ«æ•°ç›®ã€‚ç„¶åå¾—åˆ°è¾“å‡ºçš„ç»“æœscoresï¼Œä»¥åŠå¾—åˆ°é¢„æµ‹ç±»åˆ«åœ¨æ ‡ç­¾è¯å…¸ä¸­å¯¹åº”çš„æ•°å€¼predicitons
```
        with tf.name_scope('dropout'):
            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.keep_pro)

        with tf.name_scope('output'):
            w = tf.get_variable("w", shape=[num_filter_total, self.pm.num_classes],
                                initializer=tf.contrib.layers.xavier_initializer())

            b = tf.Variable(tf.constant(0.1, shape=[self.pm.num_classes]), name='b')

            self.scores = tf.nn.xw_plus_b(self.h_drop, w, b, name='scores')
            self.pro = tf.nn.softmax(self.scores)  # æœ€å¤§ä¸º1ï¼Œå…¶ä½™ä¸º0
            self.predicitions = tf.argmax(self.pro, 1, name='predictions')
```

## 3.5 loss
è¿™é‡Œä½¿ç”¨softmaxäº¤å‰ç†µæ±‚loss, logits=self.scores è¿™é‡Œä¸€å®šç”¨çš„æ˜¯æœªç»è¿‡softmaxå¤„ç†çš„æ•°å€¼ã€‚

```
        with tf.name_scope('loss'):
            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
            self.loss = tf.reduce_mean(losses)  # å¯¹äº¤å‰ç†µå–å‡å€¼éå¸¸æœ‰å¿…è¦

```
## 3.6 optimizer
è¿™é‡Œä½¿ç”¨äº†æ¢¯åº¦è£å‰ªã€‚é¦–å…ˆè®¡ç®—æ¢¯åº¦ï¼Œè¿™ä¸ªè®¡ç®—æ˜¯ç±»ä¼¼L2æ­£åˆ™åŒ–è®¡ç®—wçš„å€¼ï¼Œä¹Ÿå°±æ˜¯æ±‚å¹³æ–¹å†å¹³æ–¹æ ¹ã€‚ç„¶åä¸è®¾å®šçš„clipè£å‰ªå€¼è¿›è¡Œæ¯”è¾ƒï¼Œå¦‚æœå°äºç­‰äºclip,æ¢¯åº¦ä¸å˜ï¼›å¦‚æœå¤§äºclip,åˆ™æ¢¯åº¦*ï¼ˆclip/æ¢¯åº¦L2å€¼ï¼‰
```
        with tf.name_scope('optimizer'):
            # é€€åŒ–å­¦ä¹ ç‡ learning_rate = lr*(0.9**(global_step/10);staircase=Trueè¡¨ç¤ºæ¯decay_stepsæ›´æ–°æ¢¯åº¦
            # learning_rate = tf.train.exponential_decay(self.config.lr, global_step=self.global_step,
            # decay_steps=10, decay_rate=self.config.lr_decay, staircase=True)
            # optimizer = tf.train.AdadeltaOptimizer(learning_rate)
            # self.optimizer = optimizer.minimize(self.loss, global_step=self.global_step) #global_step è‡ªåŠ¨+1
            # no.2
            optimizer = tf.train.AdamOptimizer(self.pm.lr)
            gradients, variables = zip(*optimizer.compute_gradients(self.loss))  # è®¡ç®—å˜é‡æ¢¯åº¦ï¼Œå¾—åˆ°æ¢¯åº¦å€¼,å˜é‡
            gradients, _ = tf.clip_by_global_norm(gradients, self.pm.clip)
            # å¯¹gè¿›è¡Œl2æ­£åˆ™åŒ–è®¡ç®—ï¼Œæ¯”è¾ƒå…¶ä¸clipçš„å€¼ï¼Œå¦‚æœl2åçš„å€¼æ›´å¤§ï¼Œè®©æ¢¯åº¦*(clip/l2_g),å¾—åˆ°æ–°æ¢¯åº¦
            self.optimizer = optimizer.apply_gradients(zip(gradients, variables),
                                                       global_step=self.global_step)  # global_step è‡ªåŠ¨+1
```
## 3.7 accuracy
æœ€åï¼Œè®¡ç®—æ¨¡å‹çš„å‡†ç¡®åº¦ã€‚

```
        with tf.name_scope('accuracy'):
            correct_predictions = tf.equal(self.predicitions, tf.argmax(self.input_y, 1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float32'), name='accuracy')
```
## 3.8 è®­ç»ƒæ¨¡å‹
```
def train(model, pm, wordid, cat_to_id, dataid):
    '''model: æ˜¯cnnå¯¹è±¡'''

    tensorboard_dir = os.path.join(TENSORBOARD_DIR, 'text_cnn', make_dir_string(dataid, pm))
    save_dir = os.path.join(CHECKPOINTS, 'text_cnn', make_dir_string(dataid, pm))
    if not os.path.exists(tensorboard_dir):
        os.makedirs(tensorboard_dir)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    save_path = os.path.join(save_dir, 'best_validation')  # åœ¨è¿™é‡Œæ„å»º

    tf.summary.scalar("loss", model.loss)
    tf.summary.scalar("accuracy", model.accuracy)
    merged_summary = tf.summary.merge_all()
    writer = tf.summary.FileWriter(tensorboard_dir)
    saver = tf.train.Saver()
    session = tf.Session()
    session.run(tf.global_variables_initializer())
    writer.add_graph(session.graph)

    print("Loading Training data...")
    x_train, y_train = process(pm.train_filename, wordid, cat_to_id, pm.seq_length)
    x_val, y_val = process(pm.val_filename, wordid, cat_to_id, pm.seq_length)
    for epoch in range(pm.num_epochs):
        print('Epoch:', epoch + 1)
        num_batchs = int((len(x_train) - 1) / pm.batch_size) + 1
        batch_train = batch_iter(x_train, y_train, pm.batch_size)

        # ä¿å­˜ä¿¡æ¯ä¸ºpandas
        train_info = {"global_step": [], "loss": [], "accuracy": []}  # è®­ç»ƒä¿¡æ¯

        for x_batch, y_batch in batch_train:
            feed_dict = model.feed_data(x_batch, y_batch, pm.keep_prob)
            _, global_step, train_summary, train_loss, train_accuracy = session.run(
                [model.optimizer, model.global_step, merged_summary, model.loss, model.accuracy], feed_dict=feed_dict)
            train_info["global_step"].append(global_step)
            train_info["loss"].append(train_loss)
            train_info["accuracy"].append(train_accuracy)

            if global_step % 100 == 0:
                val_loss, val_accuracy = model.evaluate(session, x_val, y_val)
                print(global_step, train_loss, train_accuracy, val_loss, val_accuracy)

            if (global_step + 1) % num_batchs == 0:
                print("Saving model...")
                save_info(os.path.join(tensorboard_dir, "train_info.csv"), train_info)
                del train_info
                train_info = {"global_step": [], "loss": [], "accuracy": []}
                saver.save(session, save_path, global_step=global_step)

        pm.lr *= pm.lr_decay
```

æ¨¡å‹è¿­ä»£æ¬¡æ•°ä¸º5ï¼Œæ¯å®Œæˆä¸€è½®è¿­ä»£ï¼Œæ¨¡å‹ä¿å­˜ä¸€æ¬¡ã€‚å½“global_stepä¸º100çš„æ•´æ•°å€æ—¶ï¼Œè¾“å‡ºæ¨¡å‹çš„è®­ç»ƒç»“æœä»¥åŠåœ¨æµ‹è¯•é›†ä¸Šçš„æµ‹è¯•ç»“æœã€‚
